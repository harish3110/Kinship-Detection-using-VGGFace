{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Nxuqvy-V52sH",
    "outputId": "d13f31b2-688a-4315-a51a-c7b1b821a1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IGaM-Zkl609V",
    "outputId": "057de589-449d-4386-ccde-fbe6630c8163"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYZ60h_Y6wzY"
   },
   "outputs": [],
   "source": [
    "root_path = 'gdrive/My Drive/capstone_project/input/recognizing-faces-in-the-wild'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "assEblMy5-4V",
    "outputId": "d6659258-6d2b-4d73-a1df-29718a5c21e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train', '.ipynb_checkpoints', 'kernel_submission.csv', 'Data Cleaning and Preprocessing.ipynb', '.DS_Store', 'sample_submission.csv', 'combinations.txt', 'Untitled.ipynb', 'open-kimono-kinship-notebook-0-792lb.ipynb', 'train_relationships.csv', 'test.zip', 'train.zip', 'df_final.csv', 'kinship-detection-with-vgg16.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(root_path))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ruWP3EfC52sT"
   },
   "source": [
    "Necassary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "HyQRhJEB52sU",
    "outputId": "bcf91ce1-2139-4013-c517-ded5bf473e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rcmalli/keras-vggface.git\n",
      "  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-7oljf2nb\n",
      "  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-7oljf2nb\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.3.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (2.8.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (4.3.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras-vggface==0.5) (3.13)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->keras-vggface==0.5) (0.46)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vggface==0.5) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-vggface==0.5) (1.0.8)\n",
      "Building wheels for collected packages: keras-vggface\n",
      "  Building wheel for keras-vggface (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jody4tgl/wheels/36/07/46/06c25ce8e9cd396dabe151ea1d8a2bc28dafcb11321c1f3a6d\n",
      "Successfully built keras-vggface\n",
      "Installing collected packages: keras-vggface\n",
      "Successfully installed keras-vggface-0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/rcmalli/keras-vggface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y7mVHLq752sY",
    "outputId": "9af7fde5-493a-4ff3-fe5b-709702988471"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras_vggface.vggface import VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-O3h987L52sc",
    "outputId": "6896ef85-9315-46cf-92d5-4fb043e2d72b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gdrive/My Drive/capstone_project/input/recognizing-faces-in-the-wild/train/'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path = root_path + \"/train_relationships.csv\"\n",
    "train_folders_path = root_path + \"/train/\"\n",
    "val_famillies = \"F09\"\n",
    "train_folders_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a3FW_UHxQ27o",
    "outputId": "7448efa8-0adc-41d7-b272-e9e96fbb8e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gdrive/My Drive/capstone_project/input/recognizing-faces-in-the-wild/train_relationships.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2bY426b-7ZbO",
    "outputId": "3b6b8859-8e90-4c27-a295-1348f126daf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gdrive/My Drive/capstone_project/input/recognizing-faces-in-the-wild/train/'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folders_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TnXdnYFU52sg",
    "outputId": "9cf8a548-9092-4f8b-86c8-63186f2c0c56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12429"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = glob(root_path + \"/train/\" + \"*/*/*.jpg\")\n",
    "len(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbEt5mf552si"
   },
   "source": [
    "Validation set consists only family 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YOH0brK52si"
   },
   "outputs": [],
   "source": [
    "train_images = [x for x in all_images if val_famillies not in x]\n",
    "val_images = [x for x in all_images if val_famillies in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UGch8Pj52sk"
   },
   "outputs": [],
   "source": [
    "train_person_to_images_map = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4Z3fdcm52sn"
   },
   "outputs": [],
   "source": [
    "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwnc_c2552sq"
   },
   "outputs": [],
   "source": [
    "for x in train_images:\n",
    "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "for x in val_images:\n",
    "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "relationships = pd.read_csv(train_file_path)\n",
    "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
    "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
    "\n",
    "train = [x for x in relationships if val_famillies not in x[0]]\n",
    "val = [x for x in relationships if val_famillies in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NUOHAglC52su",
    "outputId": "58286f45-d10b-4536-d3e3-8d712d6e2c4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0627 10:01:58.822360 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0627 10:01:58.859759 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0627 10:01:58.867912 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0627 10:01:58.900207 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0627 10:01:58.902811 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0627 10:02:01.665771 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0627 10:02:01.735709 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0627 10:02:06.212628 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
      "94699520/94694792 [==============================] - 6s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0627 10:02:22.428784 139859425564544 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0627 10:02:22.491325 139859425564544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0627 10:02:22.501400 139859425564544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_resnet50 (Model)        multiple             23561152    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 1, 1, 2048)   0           vggface_resnet50[1][0]           \n",
      "                                                                 vggface_resnet50[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 1, 1, 2048)   0           vggface_resnet50[1][0]           \n",
      "                                                                 vggface_resnet50[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1, 1, 100)    204900      add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 1, 100)    204900      subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 1, 200)    0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 200)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          20100       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 25)           2525        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 25)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            26          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,993,603\n",
      "Trainable params: 23,940,483\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def read_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img, version=2)\n",
    "\n",
    "\n",
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "        labels = [1] * len(batch_tuples)\n",
    "        while len(batch_tuples) < batch_size:\n",
    "            p1 = choice(ppl)\n",
    "            p2 = choice(ppl)\n",
    "\n",
    "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "                batch_tuples.append((p1, p2))\n",
    "                labels.append(0)\n",
    "\n",
    "        for x in batch_tuples:\n",
    "            if not len(person_to_images_map[x[0]]):\n",
    "                print(x[0])\n",
    "\n",
    "        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "        X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "        X2 = np.array([read_img(x) for x in X2])\n",
    "\n",
    "        yield [X1, X2], labels\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    input_1 = Input(shape=(224, 224, 3))\n",
    "    input_2 = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model = VGGFace(model='resnet50', include_top=False)\n",
    "\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    x1 = base_model(input_1)\n",
    "    x2 = base_model(input_2)\n",
    "\n",
    "\n",
    "    merged_add = Add()([x1, x2])\n",
    "    merged_sub = Subtract()([x1,x2])\n",
    "    \n",
    "    merged_add = Conv2D(100 , [1,1] )(merged_add)\n",
    "    merged_sub = Conv2D(100 , [1,1] )(merged_sub)\n",
    "    \n",
    "    merged = Concatenate(axis=-1)([merged_add, merged_sub])\n",
    "    \n",
    "    merged = Flatten()(merged)\n",
    "\n",
    "    merged = Dense(100, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    merged = Dense(25, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "file_path = \"vgg_face.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759
    },
    "colab_type": "code",
    "id": "73F7GJss52sx",
    "outputId": "778e4682-f0a6-4f87-a8f2-060992551deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 591s 3s/step - loss: 1.2386 - acc: 0.5375 - val_loss: 0.7275 - val_acc: 0.5962\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59625, saving model to vgg_face.h5\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 275s 1s/step - loss: 0.7998 - acc: 0.5759 - val_loss: 0.6804 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59625 to 0.61375, saving model to vgg_face.h5\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 181s 905ms/step - loss: 0.7131 - acc: 0.5850 - val_loss: 0.6547 - val_acc: 0.6244\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.61375 to 0.62438, saving model to vgg_face.h5\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 137s 686ms/step - loss: 0.7023 - acc: 0.6028 - val_loss: 0.6414 - val_acc: 0.6425\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.62438 to 0.64250, saving model to vgg_face.h5\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 112s 559ms/step - loss: 0.6677 - acc: 0.6131 - val_loss: 0.6265 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64250\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 104s 522ms/step - loss: 0.6375 - acc: 0.6288 - val_loss: 0.6151 - val_acc: 0.6644\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.64250 to 0.66438, saving model to vgg_face.h5\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 104s 518ms/step - loss: 0.6157 - acc: 0.6534 - val_loss: 0.6232 - val_acc: 0.6488\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.66438\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 104s 521ms/step - loss: 0.6297 - acc: 0.6497 - val_loss: 0.6116 - val_acc: 0.6637\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.66438\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 104s 519ms/step - loss: 0.6158 - acc: 0.6472 - val_loss: 0.6008 - val_acc: 0.6769\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.66438 to 0.67688, saving model to vgg_face.h5\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 103s 516ms/step - loss: 0.6116 - acc: 0.6459 - val_loss: 0.6082 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.67688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f333dae5c50>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n",
    "                    validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=10, verbose=1,\n",
    "                    workers = 4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sM79WE3l52sz",
    "outputId": "276bafd8-66f1-46a5-c5ec-e9427daca13c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [31:47, 23.07s/it]"
     ]
    }
   ],
   "source": [
    "test_path = root_path + \"/test/\"\n",
    "\n",
    "\n",
    "def chunker(seq, size=32):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "submission = pd.read_csv(root_path + '/sample_submission.csv')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in tqdm(chunker(submission.img_pair.values)):\n",
    "    X1 = [x.split(\"-\")[0] for x in batch]\n",
    "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
    "\n",
    "    X2 = [x.split(\"-\")[1] for x in batch]\n",
    "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
    "\n",
    "    pred = model.predict([X1, X2]).ravel().tolist()\n",
    "    predictions += pred\n",
    "\n",
    "submission['is_related'] = predictions\n",
    "\n",
    "submission.to_csv(\"vgg_face.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njrRZXI7kda1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kinship_detection_with_vgg16.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
